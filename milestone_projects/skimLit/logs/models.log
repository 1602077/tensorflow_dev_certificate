--------------------------------------------------
  Model0_TF-IDF-Multinomial-Naive-Bayes
--------------------------------------------------
Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())])

Baseline Model Results
{ 'accuracy': 72.1832384482987,
  'f1': 0.6989250353450294,
  'precision': 0.7186466952323352,
  'recall': 0.7218323844829869}

--------------------------------------------------
  Preparing data for deep learning
--------------------------------------------------
Average length of each sentence: 26.34
Most sentences are around ~25 words long.
95% percentile sentence length: 55
Maximum sentence length in training set: 296

--------------------------------------------------
  Model1_Conv_1D
--------------------------------------------------
Model: "Model1_Conv_1D"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
text_vectorization (TextVect (None, 55)                0         
_________________________________________________________________
token_embedding (Embedding)  (None, 55, 128)           8299648   
_________________________________________________________________
conv1d (Conv1D)              (None, 55, 64)            41024     
_________________________________________________________________
global_average_pooling1d (Gl (None, 64)                0         
_________________________________________________________________
dense (Dense)                (None, 5)                 325       
=================================================================
Total params: 8,340,997
Trainable params: 8,340,997
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
562/562 - 20s - loss: 0.9210 - accuracy: 0.6343 - val_loss: 0.6856 - val_accuracy: 0.7407
Epoch 2/3
562/562 - 20s - loss: 0.6576 - accuracy: 0.7529 - val_loss: 0.6239 - val_accuracy: 0.7766
Epoch 3/3
562/562 - 19s - loss: 0.6169 - accuracy: 0.7734 - val_loss: 0.5947 - val_accuracy: 0.7862

Model 1 Results:
{'accuracy': 78.70382629418773,
 'f1': 0.7843516226673608,
 'precision': 0.7837197264346222,
 'recall': 0.7870382629418774}

--------------------------------------------------
  Creating embeddings using USE feature extractor from tf_hub
--------------------------------------------------
Random sentence: four hundred and twelve and @ participants received amoxicillin thrice or twice daily , respectively .
Sentence after embedding (first 30 values only):
 [-0.03752713 -0.04149889  0.01297536 -0.02361394 -0.07525036  0.07473115
  0.0011491  -0.06755162 -0.07916349 -0.03044743  0.06086067  0.00604124
  0.05118055 -0.04848755  0.02337787 -0.02308349 -0.07547472  0.03421701
 -0.02380681  0.03020227  0.08559693 -0.0162654  -0.08270311  0.03540774
 -0.03399321  0.06043173 -0.00887496 -0.00420454  0.04764642  0.04315717]

Length of sentence embedding: 512

--------------------------------------------------
  Model2_USE_FeatExtr
--------------------------------------------------
Model: "Model2_USE_FeatExtr"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None,)]                 0         
_________________________________________________________________
universal_sentence_encoder ( (None, 512)               256797824 
_________________________________________________________________
dense_1 (Dense)              (None, 128)               65664     
_________________________________________________________________
dense_2 (Dense)              (None, 5)                 645       
=================================================================
Total params: 256,864,133
Trainable params: 66,309
Non-trainable params: 256,797,824
_________________________________________________________________
Epoch 1/3
562/562 - 24s - loss: 0.9175 - accuracy: 0.6499 - val_loss: 0.7979 - val_accuracy: 0.6898
Epoch 2/3
562/562 - 23s - loss: 0.7698 - accuracy: 0.7003 - val_loss: 0.7570 - val_accuracy: 0.7038
Epoch 3/3
562/562 - 23s - loss: 0.7531 - accuracy: 0.7114 - val_loss: 0.7407 - val_accuracy: 0.7148

Model 2 Results:
{'accuracy': 71.32927313650205,
 'f1': 0.7102782745684071,
 'precision': 0.7140404812009038,
 'recall': 0.7132927313650205}

--------------------------------------------------
  Creating character embeddings
--------------------------------------------------
Average character length: 149.3662574983337
Sequence length which covers 95% of sentences: 290
Possible characters: abcdefghijklmnopqrstuvwxyz0123456789!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~
Number of chars in vocab: 28
Top 5 chars: ['', '[UNK]', 'e', 't', 'i']
Bottom 5 chars: ['k', 'x', 'z', 'q', 'j']

--------------------------------------------------
  Model3_Conv1D_CharEmbed
--------------------------------------------------
Model: "Model3_Conv1D_CharEmbed"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
char_vectoriser (TextVectori (None, 55)                0         
_________________________________________________________________
char_embed (Embedding)       (None, 55, 25)            700       
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 55, 64)            8064      
_________________________________________________________________
global_max_pooling1d (Global (None, 64)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 325       
=================================================================
Total params: 9,089
Trainable params: 9,089
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
562/562 - 6s - loss: 1.3025 - accuracy: 0.4558 - val_loss: 1.1482 - val_accuracy: 0.5362
Epoch 2/3
562/562 - 5s - loss: 1.1103 - accuracy: 0.5498 - val_loss: 1.0528 - val_accuracy: 0.5785
Epoch 3/3
562/562 - 5s - loss: 1.0353 - accuracy: 0.5852 - val_loss: 0.9896 - val_accuracy: 0.6157

Model 3 Results:
{'accuracy': 60.50575930094002,
 'f1': 0.5924606385924093,
 'precision': 0.5992022153134922,
 'recall': 0.6050575930094002}

--------------------------------------------------
  Model4_TokenAndCharEmbed
--------------------------------------------------
Model: "Model4_TokenAndCharEmbed"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
char_input (InputLayer)         [(None, 1)]          0                                            
__________________________________________________________________________________________________
token_inputs (InputLayer)       [(None,)]            0                                            
__________________________________________________________________________________________________
char_vectoriser (TextVectorizat (None, 55)           0           char_input[0][0]                 
__________________________________________________________________________________________________
universal_sentence_encoder (Ker (None, 512)          256797824   token_inputs[0][0]               
__________________________________________________________________________________________________
char_embed (Embedding)          (None, 55, 25)       700         char_vectoriser[1][0]            
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 128)          65664       universal_sentence_encoder[1][0] 
__________________________________________________________________________________________________
bidirectional (Bidirectional)   (None, 48)           9600        char_embed[1][0]                 
__________________________________________________________________________________________________
token_char_hybrid (Concatenate) (None, 176)          0           dense_4[0][0]                    
                                                                 bidirectional[0][0]              
__________________________________________________________________________________________________
dropout (Dropout)               (None, 176)          0           token_char_hybrid[0][0]          
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 128)          22656       dropout[0][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 128)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 5)            645         dropout_1[0][0]                  
==================================================================================================
Total params: 256,897,089
Trainable params: 99,265
Non-trainable params: 256,797,824
__________________________________________________________________________________________________
Check prefetch train and token datasets
<PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.float64)>
<PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.float64)>
Epoch 1/3
562/562 - 79s - loss: 1.0009 - accuracy: 0.5943 - val_loss: 0.8112 - val_accuracy: 0.6878
Epoch 2/3
562/562 - 73s - loss: 0.8234 - accuracy: 0.6778 - val_loss: 0.7410 - val_accuracy: 0.7151
Epoch 3/3
562/562 - 74s - loss: 0.7979 - accuracy: 0.6963 - val_loss: 0.7124 - val_accuracy: 0.7307

Model 4 Results:
{'accuracy': 72.44472395074804,
 'f1': 0.7209823728238538,
 'precision': 0.7268846995097742,
 'recall': 0.7244472395074805}

--------------------------------------------------
  Model5_Tri_Embed_Model
--------------------------------------------------
Number of different line numbers
Train line numbers one-hot [:10]:
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
Shape: (180040, 15)
Length coverage 95% of total_lienes: 18.0
Train total line numbers one-hot [:10]:
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]
Shape: (180040, 20)
Model: "Model5_Tri_Embed_Model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
char_inputs (InputLayer)        [(None, 1)]          0                                            
__________________________________________________________________________________________________
token_inputs (InputLayer)       [(None,)]            0                                            
__________________________________________________________________________________________________
char_vectoriser (TextVectorizat (None, 55)           0           char_inputs[0][0]                
__________________________________________________________________________________________________
universal_sentence_encoder (Ker (None, 512)          256797824   token_inputs[0][0]               
__________________________________________________________________________________________________
char_embed (Embedding)          (None, 55, 25)       700         char_vectoriser[2][0]            
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 128)          65664       universal_sentence_encoder[2][0] 
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 48)           9600        char_embed[2][0]                 
__________________________________________________________________________________________________
M5_char_token_hybrid_embedding  (None, 176)          0           dense_7[0][0]                    
                                                                 bidirectional_1[0][0]            
__________________________________________________________________________________________________
line_num_inputs (InputLayer)    [(None, 15)]         0                                            
__________________________________________________________________________________________________
total_num_inputs (InputLayer)   [(None, 20)]         0                                            
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 256)          45312       M5_char_token_hybrid_embedding[0]
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 32)           512         line_num_inputs[0][0]            
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 32)           672         total_num_inputs[0][0]           
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 256)          0           dense_10[0][0]                   
__________________________________________________________________________________________________
M5_char_token_pos_embed (Concat (None, 320)          0           dense_8[0][0]                    
                                                                 dense_9[0][0]                    
                                                                 dropout_2[0][0]                  
__________________________________________________________________________________________________
output_layer (Dense)            (None, 5)            1605        M5_char_token_pos_embed[0][0]    
==================================================================================================
Total params: 256,921,889
Trainable params: 124,065
Non-trainable params: 256,797,824
__________________________________________________________________________________________________
Epoch 1/3
562/562 - 81s - loss: 1.0947 - accuracy: 0.7201 - val_loss: 0.9900 - val_accuracy: 0.7975
Epoch 2/3
562/562 - 77s - loss: 0.9718 - accuracy: 0.8116 - val_loss: 0.9586 - val_accuracy: 0.8215
Epoch 3/3
562/562 - 78s - loss: 0.9559 - accuracy: 0.8180 - val_loss: 0.9481 - val_accuracy: 0.8231

Model 5 Results:
{'accuracy': 82.71878723685953,
 'f1': 0.8263584954304537,
 'precision': 0.8259908254171083,
 'recall': 0.8271878723685953}

--------------------------------------------------
  Model6_TribridModel_FullDataset
--------------------------------------------------
Model: "Model6_TribridModel_FullDataset"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
char_inputs (InputLayer)        [(None, 1)]          0                                            
__________________________________________________________________________________________________
token_inputs (InputLayer)       [(None,)]            0                                            
__________________________________________________________________________________________________
char_vectoriser (TextVectorizat (None, 55)           0           char_inputs[0][0]                
__________________________________________________________________________________________________
universal_sentence_encoder (Ker (None, 512)          256797824   token_inputs[0][0]               
__________________________________________________________________________________________________
char_embed (Embedding)          (None, 55, 25)       700         char_vectoriser[0][0]            
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 128)          65664       universal_sentence_encoder[0][0] 
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 48)           9600        char_embed[0][0]                 
__________________________________________________________________________________________________
M5_char_token_hybrid_embedding  (None, 176)          0           dense_7[0][0]                    
                                                                 bidirectional_1[0][0]            
__________________________________________________________________________________________________
line_num_inputs (InputLayer)    [(None, 15)]         0                                            
__________________________________________________________________________________________________
total_num_inputs (InputLayer)   [(None, 20)]         0                                            
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 256)          45312       M5_char_token_hybrid_embedding[0]
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 32)           512         line_num_inputs[0][0]            
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 32)           672         total_num_inputs[0][0]           
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 256)          0           dense_10[0][0]                   
__________________________________________________________________________________________________
M5_char_token_pos_embed (Concat (None, 320)          0           dense_8[0][0]                    
                                                                 dense_9[0][0]                    
                                                                 dropout_2[0][0]                  
__________________________________________________________________________________________________
output_layer (Dense)            (None, 5)            1605        M5_char_token_pos_embed[0][0]    
==================================================================================================
Total params: 256,921,889
Trainable params: 124,065
Non-trainable params: 256,797,824
__________________________________________________________________________________________________
Epoch 1/3
5627/5627 - 727s - loss: 0.9585 - accuracy: 0.8189 - val_loss: 0.9137 - val_accuracy: 0.8442
Epoch 2/3
5627/5627 - 672s - loss: 0.9141 - accuracy: 0.8496 - val_loss: 0.9070 - val_accuracy: 0.8466
Epoch 3/3
5627/5627 - 23871s - loss: 0.9018 - accuracy: 0.8582 - val_loss: 0.9028 - val_accuracy: 0.8489

Model 6 Results:
{'accuracy': 84.89010989010988,
 'f1': 0.8458169010566923,
 'precision': 0.8489645955780886,
 'recall': 0.8489010989010989}

--------------------------------------------------
  MODEL PERFORMANCE SUMMARIES
--------------------------------------------------

                                       accuracy  precision    recall        f1
Model0_TF-IDF-Multinomial-Naive-Bayes  0.721832   0.718647  0.721832  0.698925
Model1_Conv_1D                         0.787038   0.783720  0.787038  0.784352
Model2_USE_FeatExtr                    0.713293   0.714040  0.713293  0.710278
Model3_Conv1D_CharEmbed                0.605058   0.599202  0.605058  0.592461
Model4_TokenAndCharEmbed               0.724447   0.726885  0.724447  0.720982
Model5_Tri_Embed_Model                 0.827188   0.825991  0.827188  0.826358
Model6_TribridModel_FullDataset        0.848901   0.848965  0.848901  0.845817
